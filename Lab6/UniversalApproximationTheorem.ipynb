{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ffG9xYmthjC"
   },
   "source": [
    "# **The universal approximation theorem**\n",
    "\n",
    "**Disclaimer**: large parts of the lab are taken from [this webpage](https://towardsdatascience.com/understand-universal-approximation-theorem-with-code-774dcef55731).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wsm7e4Witd12"
   },
   "source": [
    "Let us test the ability of standard single-hidden-layer feedforward Neural Network of finite number of hidden neurons of approximating continuous functions by means of an arbitrary activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7jEvpdaArWOf"
   },
   "source": [
    "The theorem has its variants related to the number of layers for fixed number of neurons. Or, for fixed number of neurons and layers, one can choose a proper activation function (is it really feasible?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-oSlK5tuEOg"
   },
   "source": [
    "We will focus on changing the structure of the problem in order to get a more \"connected\" Neural Network able to better capture the features of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4bfLCIKdn_Qs"
   },
   "outputs": [],
   "source": [
    "# Downloading dependencies:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OfzHN_k7uWP8"
   },
   "outputs": [],
   "source": [
    "### Setting function to approximate ###\n",
    "\n",
    "# y = x^2\n",
    "x = np.linspace(-30,30,100)\n",
    "y = x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHIOmwgrDos2"
   },
   "source": [
    "Working on the universal approximation theorem means _changing_ the parameters and the structure of the net:\n",
    "* number of neurons\n",
    "* number of layers\n",
    "* learning rate (very important!) and optimization process ([LOOK!](https://imgur.com/a/Hqolp))\n",
    "* number of epochs (duration of the training process) \n",
    "* activation function (.... _really?_ ....)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5aOa9JTXuqki"
   },
   "outputs": [],
   "source": [
    "# Net definition #\n",
    "\n",
    "n_neurons = 10  # number of neurons/nodes\n",
    "learning_rate = 5e-3 # learning rate\n",
    "\n",
    "   \n",
    "model = nn.Sequential(     \n",
    "          nn.Linear(1, n_neurons),\n",
    "          nn.ReLU(),\n",
    "          # nn.Linear(n_neurons,n_neurons),\n",
    "          # nn.ReLU(),        \n",
    "          nn.Linear(n_neurons,1),\n",
    "          nn.ReLU()\n",
    "          )\n",
    "\n",
    "\n",
    "# Set up  : Input (1 Node) -> Hidden (n_neurons nodes) -> Output (1 Node) \n",
    "# Set up 2: Input (1 Node) -> Hidden (n_neurons nodes) -> Hidden (n_neurons nodes) -> Output (1 Node)\n",
    "\n",
    "# Important Note: If you increase the number of neurons or use a harder function to approximate, try tuning the learning rate.\n",
    "#                 Tuning the learning rate is vital to properly train the network.\n",
    "\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=learning_rate) # define optimizer\n",
    "# optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "criterion = nn.MSELoss() # define loss function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AcXA_pTHCMQ"
   },
   "source": [
    "Let us train the net!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D1ZTg7Mju0ze"
   },
   "outputs": [],
   "source": [
    "# Training #\n",
    "\n",
    "# Convert to tensor form with batch for PyTorch model.\n",
    "inputs = torch.tensor(x).view(-1,1)\n",
    "labels = torch.tensor(y).view(-1,1)\n",
    "\n",
    "# Important Note 2: Change epochs\n",
    "epochs = 20000\n",
    "\n",
    "for epoch in range(epochs):  # loop over the data multiple times\n",
    "   \n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "   \n",
    "    # forward + backward + optimize\n",
    "    outputs = model(inputs.float())\n",
    "    loss = criterion(outputs, labels.float())\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ee9CXJ6DHOAE"
   },
   "source": [
    "Let us test the model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "vYFQM167u61-",
    "outputId": "4a41a273-bfdf-43a2-b6f7-6118bfc3be71"
   },
   "outputs": [],
   "source": [
    "### Running Inference over the trained model ***********  #\n",
    "x_test = np.linspace(-50,50,50)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_inputs = torch.tensor(x_test).view(len(x_test),-1).float()\n",
    "    y_hat = model(test_inputs)\n",
    "    y_hat = y_hat.detach().numpy()\n",
    "    \n",
    "# ******************************************************  #\n",
    "\n",
    "### Plot results: Actual vs Model Prediction ***********  #\n",
    "plt.scatter(x,y,label='Actual Function')\n",
    "plt.scatter(x_test,y_hat,label=\"Predicted Function\")\n",
    "plt.title(f'Number of neurons: {n_neurons}')\n",
    "plt.xlabel('Input Variable (x)')\n",
    "plt.ylabel('Output Variable (y)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# ******************************************************  #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLfzOAj_HRHY"
   },
   "source": [
    "Try other functions and configurations!\n",
    "\n",
    "**Enjoy!**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
